{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot-keras.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jg-Nascimento/chatabot_flask/blob/main/chatbot_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siKbYLepFGCG"
      },
      "source": [
        "# Chatbot using Keras\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzjdQzL1xi4O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5112a408-a85d-4fc3-9c48-3f5d29e2cab8"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import time\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
        "import random\n",
        "import nltk\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg3C47NWEikT"
      },
      "source": [
        "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 56} ) \n",
        "sess = tf.Session(config=config) \n",
        "tf.keras.backend.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo8nyLb-cMp8"
      },
      "source": [
        "import requests, zipfile, io\n",
        "\n",
        "r = requests.get('http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip') \n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_NhYsGOE_zu"
      },
      "source": [
        "## Pre Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJHDaK5Rc1gr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b19a2602-80b5-4c02-8383-45bd1fb74172"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'cornell movie-dialogs corpus'\t __MACOSX   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJEUtE4Uy6ek"
      },
      "source": [
        "lines = open('cornell movie-dialogs corpus/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "conversations = open('cornell movie-dialogs corpus/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hOWfvYmy-O6"
      },
      "source": [
        "EN_WHITELIST = '0123456789abcdefghijklmnopqrstuvwxyz ' # space is included in whitelist\n",
        "EN_BLACKLIST = '!\"#$%&\\'()*+,-./:;=?@[\\\\]^_`{|}~\\''\n",
        "\n",
        "limit = {\n",
        "        'maxq' : 10,\n",
        "        'minq' : 2,\n",
        "        'maxa' : 10,\n",
        "        'mina' : 2\n",
        "        }\n",
        "\n",
        "UNK = 'unk'\n",
        "VOCAB_SIZE = 5000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okVkOZpsz0S3"
      },
      "source": [
        "'''\n",
        "    1. Read from 'movie-lines.txt'\n",
        "    2. Create a dictionary with ( key = line_id, value = text )\n",
        "'''\n",
        "def get_id2line():\n",
        "    lines=open('cornell movie-dialogs corpus/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "    id2line = {}\n",
        "    for line in lines:\n",
        "        _line = line.split(' +++$+++ ')\n",
        "        if len(_line) == 5:\n",
        "            id2line[_line[0]] = _line[4]\n",
        "    return id2line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMZuqopWxwvY"
      },
      "source": [
        "'''\n",
        "    1. Read from 'movie_conversations.txt'\n",
        "    2. Create a list of [list of line_id's]\n",
        "'''\n",
        "def get_conversations():\n",
        "    conv_lines = open('cornell movie-dialogs corpus/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "    convs = [ ]\n",
        "    for line in conv_lines[:-1]:\n",
        "        _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "        convs.append(_line.split(','))\n",
        "    return convs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvGFEk2wdOgc"
      },
      "source": [
        "'''\n",
        "    1. Get each conversation\n",
        "    2. Get each line from conversation\n",
        "    3. Save each conversation to file\n",
        "'''\n",
        "def extract_conversations(convs,id2line,path=''):\n",
        "    idx = 0\n",
        "    for conv in convs:\n",
        "        f_conv = open(path + str(idx)+'.txt', 'w')\n",
        "        for line_id in conv:\n",
        "            f_conv.write(id2line[line_id])\n",
        "            f_conv.write('\\n')\n",
        "        f_conv.close()\n",
        "        idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df86LBDSdS4V"
      },
      "source": [
        "'''\n",
        "    Get lists of all conversations as Questions and Answers\n",
        "    1. [questions]\n",
        "    2. [answers]\n",
        "'''\n",
        "def gather_dataset(convs, id2line):\n",
        "    questions = []; answers = []\n",
        "\n",
        "    for conv in convs:\n",
        "        if len(conv) %2 != 0:\n",
        "            conv = conv[:-1]\n",
        "        for i in range(len(conv)):\n",
        "            if i%2 == 0:\n",
        "                questions.append(id2line[conv[i]])\n",
        "            else:\n",
        "                answers.append(id2line[conv[i]])\n",
        "\n",
        "    return questions, answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJyz4zOzdW4i"
      },
      "source": [
        "'''\n",
        " remove anything that isn't in the vocabulary\n",
        "    return str(pure en)\n",
        "\n",
        "'''\n",
        "def filter_line(line, whitelist):\n",
        "    return ''.join([ ch for ch in line if ch in whitelist ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvForFWpdZ_O"
      },
      "source": [
        "'''\n",
        " filter too long and too short sequences\n",
        "    return tuple( filtered_ta, filtered_en )\n",
        "\n",
        "'''\n",
        "def filter_data(qseq, aseq):\n",
        "    filtered_q, filtered_a = [], []\n",
        "    raw_data_len = len(qseq)\n",
        "\n",
        "    assert len(qseq) == len(aseq)\n",
        "\n",
        "    for i in range(raw_data_len):\n",
        "        qlen, alen = len(qseq[i].split(' ')), len(aseq[i].split(' '))\n",
        "        if qlen >= limit['minq'] and qlen <= limit['maxq']:\n",
        "            if alen >= limit['mina'] and alen <= limit['maxa']:\n",
        "                filtered_q.append(qseq[i])\n",
        "                filtered_a.append(aseq[i])\n",
        "\n",
        "    # print the fraction of the original data, filtered\n",
        "    filt_data_len = len(filtered_q)\n",
        "    filtered = int((raw_data_len - filt_data_len)*100/raw_data_len)\n",
        "    print(str(filtered) + '% filtered from original data')\n",
        "\n",
        "    return filtered_q, filtered_a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky7Ki6GTddWC"
      },
      "source": [
        "'''\n",
        " read list of words, create index to word,\n",
        "  word to index dictionaries\n",
        "    return tuple( vocab->(word, count), idx2w, w2idx )\n",
        "\n",
        "'''\n",
        "def index_(tokenized_sentences, vocab_size):\n",
        "    # get frequency distribution\n",
        "    freq_dist = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "    # get vocabulary of 'vocab_size' most used words\n",
        "    vocab = freq_dist.most_common(vocab_size)\n",
        "    # index2word\n",
        "    index2word = ['_'] + [UNK] + [x[0] for x in vocab]\n",
        "    # word2index\n",
        "    word2index = dict([(w,i) for i,w in enumerate(index2word)] )\n",
        "    return index2word, word2index, freq_dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYKkJqjjdggf"
      },
      "source": [
        "'''\n",
        " filter based on number of unknowns (words not in vocabulary)\n",
        "  filter out the worst sentences\n",
        "\n",
        "'''\n",
        "def filter_unk(qtokenized, atokenized, w2idx):\n",
        "    data_len = len(qtokenized)\n",
        "\n",
        "    filtered_q, filtered_a = [], []\n",
        "\n",
        "    for qline, aline in zip(qtokenized, atokenized):\n",
        "        unk_count_q = len([ w for w in qline if w not in w2idx ])\n",
        "        unk_count_a = len([ w for w in aline if w not in w2idx ])\n",
        "        if unk_count_a <= 2:\n",
        "            if unk_count_q > 0:\n",
        "                if unk_count_q/len(qline) > 0.2:\n",
        "                    pass\n",
        "            filtered_q.append(qline)\n",
        "            filtered_a.append(aline)\n",
        "\n",
        "    # print the fraction of the original data, filtered\n",
        "    filt_data_len = len(filtered_q)\n",
        "    filtered = int((data_len - filt_data_len)*100/data_len)\n",
        "    print(str(filtered) + '% filtered from original data')\n",
        "\n",
        "    return filtered_q, filtered_a\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI3-crBjdjiL"
      },
      "source": [
        "'''\n",
        " create the final dataset :\n",
        "  - convert list of items to arrays of indices\n",
        "  - add zero padding\n",
        "      return ( [array_en([indices]), array_ta([indices]) )\n",
        "\n",
        "'''\n",
        "def zero_pad(qtokenized, atokenized, w2idx):\n",
        "    # num of rows\n",
        "    data_len = len(qtokenized)\n",
        "\n",
        "    # numpy arrays to store indices\n",
        "    idx_q = np.zeros([data_len, limit['maxq']], dtype=np.int32)\n",
        "    idx_a = np.zeros([data_len, limit['maxa']], dtype=np.int32)\n",
        "\n",
        "    for i in range(data_len):\n",
        "        q_indices = pad_seq(qtokenized[i], w2idx, limit['maxq'])\n",
        "        a_indices = pad_seq(atokenized[i], w2idx, limit['maxa'])\n",
        "\n",
        "        #print(len(idx_q[i]), len(q_indices))\n",
        "        #print(len(idx_a[i]), len(a_indices))\n",
        "        idx_q[i] = np.array(q_indices)\n",
        "        idx_a[i] = np.array(a_indices)\n",
        "\n",
        "    return idx_q, idx_a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEJC342sdoBZ"
      },
      "source": [
        "'''\n",
        " replace words with indices in a sequence\n",
        "  replace with unknown if word not in lookup\n",
        "    return [list of indices]\n",
        "\n",
        "'''\n",
        "def pad_seq(seq, lookup, maxlen):\n",
        "    indices = []\n",
        "    for word in seq:\n",
        "        if word in lookup:\n",
        "            indices.append(lookup[word])\n",
        "        else:\n",
        "            indices.append(lookup[UNK])\n",
        "    return indices + [0]*(maxlen - len(seq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9faYltMHFXUt"
      },
      "source": [
        "## Pre processing output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Vp2heodvBg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "027cd937-fe26-4f44-89d9-c2a54f497c94"
      },
      "source": [
        "def process_data():\n",
        "\n",
        "    id2line = get_id2line()\n",
        "    print('>> gathered id2line dictionary.\\n')\n",
        "    convs = get_conversations()\n",
        "    print(convs[121:125])\n",
        "    print('>> gathered conversations.\\n')\n",
        "    questions, answers = gather_dataset(convs,id2line)\n",
        "\n",
        "    # change to lower case (just for en)\n",
        "    # Add sos and eos \n",
        "    questions = [ line.lower() for line in questions ]\n",
        "    answers = [ '<sos> ' + line.lower() + ' <eos>'for line in answers ]\n",
        "\n",
        "    # filter out unnecessary characters\n",
        "    print('\\n>> Filter lines')\n",
        "    questions = [ filter_line(line, EN_WHITELIST) for line in questions ]\n",
        "    answers = [ filter_line(line, EN_WHITELIST) for line in answers ]\n",
        "\n",
        "    # filter out too long or too short sequences\n",
        "    print('\\n>> 2nd layer of filtering')\n",
        "    qlines, alines = filter_data(questions, answers)\n",
        "\n",
        "    for q,a in zip(qlines[141:145], alines[141:145]):\n",
        "        print('q : [{0}]; a : [{1}]'.format(q,a))\n",
        "\n",
        "    # convert list of [lines of text] into list of [list of words ]\n",
        "    print('\\n>> Segment lines into words')\n",
        "    qtokenized = [ [w.strip() for w in wordlist.split(' ') if w] for wordlist in qlines ]\n",
        "    atokenized = [ [w.strip() for w in wordlist.split(' ') if w] for wordlist in alines ]\n",
        "    print('\\n:: Sample from segmented list of words')\n",
        "\n",
        "    for q,a in zip(qtokenized[141:145], atokenized[141:145]):\n",
        "        print('q : [{0}]; a : [{1}]'.format(q,a))\n",
        "\n",
        "    # indexing -> idx2w, w2idx\n",
        "    print('\\n >> Index words')\n",
        "    idx2w, w2idx, freq_dist = index_( qtokenized + atokenized, vocab_size=VOCAB_SIZE)\n",
        "\n",
        "    # filter out sentences with too many unknowns\n",
        "    print('\\n >> Filter Unknowns')\n",
        "    qtokenized, atokenized = filter_unk(qtokenized, atokenized, w2idx)\n",
        "    print('\\n Final dataset len : ' + str(len(qtokenized)))\n",
        "\n",
        "\n",
        "    print('\\n >> Zero Padding')\n",
        "    idx_q, idx_a = zero_pad(qtokenized, atokenized, w2idx)\n",
        "\n",
        "    print('\\n >> Save numpy arrays to disk')\n",
        "    # save them\n",
        "    np.save('idx_q.npy', idx_q)\n",
        "    np.save('idx_a.npy', idx_a)\n",
        "\n",
        "    # let us now save the necessary dictionaries\n",
        "    metadata = {\n",
        "            'w2idx' : w2idx,\n",
        "            'idx2w' : idx2w,\n",
        "            'limit' : limit,\n",
        "            'freq_dist' : freq_dist\n",
        "                }\n",
        "\n",
        "    # write to disk : data control dictionaries\n",
        "    with open('metadata.pkl', 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "    # count of unknowns\n",
        "    unk_count = (idx_q == 1).sum() + (idx_a == 1).sum()\n",
        "    # count of words\n",
        "    word_count = (idx_q > 1).sum() + (idx_a > 1).sum()\n",
        "\n",
        "    print('% unknown : {0}'.format(100 * (unk_count/word_count)))\n",
        "    print('Dataset count : ' + str(idx_q.shape[0]))\n",
        "\n",
        "\n",
        "process_data()\n",
        "\n",
        "\n",
        "def load_data(PATH=''):\n",
        "    # read data control dictionaries\n",
        "    with open(PATH + 'metadata.pkl', 'rb') as f:\n",
        "        metadata = pickle.load(f)\n",
        "    # read numpy arrays\n",
        "    idx_q = np.load(PATH + 'idx_q.npy')\n",
        "    idx_a = np.load(PATH + 'idx_a.npy')\n",
        "    return metadata, idx_q, idx_a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> gathered id2line dictionary.\n",
            "\n",
            "[['L447', 'L448'], ['L490', 'L491'], ['L716', 'L717', 'L718', 'L719', 'L720', 'L721'], ['L750', 'L751', 'L752', 'L753', 'L754', 'L755']]\n",
            ">> gathered conversations.\n",
            "\n",
            "\n",
            ">> Filter lines\n",
            "\n",
            ">> 2nd layer of filtering\n",
            "63% filtered from original data\n",
            "q : [i want to go with you]; a : [sos therell be a time eos]\n",
            "q : [do you swear on all the holy saints in heaven]; a : [sos yes yes i do on all of them eos]\n",
            "q : [i have to explore the mainland]; a : [sos this time with me eos]\n",
            "q : [how are you feeling fernando]; a : [sos not bad eos]\n",
            "\n",
            ">> Segment lines into words\n",
            "\n",
            ":: Sample from segmented list of words\n",
            "q : [['i', 'want', 'to', 'go', 'with', 'you']]; a : [['sos', 'therell', 'be', 'a', 'time', 'eos']]\n",
            "q : [['do', 'you', 'swear', 'on', 'all', 'the', 'holy', 'saints', 'in', 'heaven']]; a : [['sos', 'yes', 'yes', 'i', 'do', 'on', 'all', 'of', 'them', 'eos']]\n",
            "q : [['i', 'have', 'to', 'explore', 'the', 'mainland']]; a : [['sos', 'this', 'time', 'with', 'me', 'eos']]\n",
            "q : [['how', 'are', 'you', 'feeling', 'fernando']]; a : [['sos', 'not', 'bad', 'eos']]\n",
            "\n",
            " >> Index words\n",
            "\n",
            " >> Filter Unknowns\n",
            "0% filtered from original data\n",
            "\n",
            " Final dataset len : 49824\n",
            "\n",
            " >> Zero Padding\n",
            "\n",
            " >> Save numpy arrays to disk\n",
            "% unknown : 4.750246969353306\n",
            "Dataset count : 49824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FumsAMmpdx8R"
      },
      "source": [
        "metadata, idx_q, idx_a = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5iTPXaad1AQ"
      },
      "source": [
        "word_dict = metadata['w2idx']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tAfay_kYs20"
      },
      "source": [
        "index_dict = metadata['idx2w']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0z1dY82eczm"
      },
      "source": [
        "MAX_NB_WORDS = 100\n",
        "MAX_SEQUENCE_LENGTH = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njNYojzhFexL"
      },
      "source": [
        "## Target with teacher forcing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG4QmoyvGgtb"
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "def target_answer(item):\n",
        "    item = deque(item)\n",
        "    item.rotate(-1)  \n",
        "    item[-1] = 0\n",
        "    return item"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M1vUSQcGjAp"
      },
      "source": [
        "decoder_target_data =  np.array([target_answer(i) for i in idx_a])\n",
        "encoder_input_data = idx_q\n",
        "decoder_input_data = idx_a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z63WG8ghgrgF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03a2ca76-26b9-4197-b274-35c4657055df"
      },
      "source": [
        "decoder_input_data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  2, 128,  12,  67,   9,   5,  61,  14,   3,   0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Itk646vZgppV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b85dfe38-c90b-4544-e4e1-9cdf2f58da7a"
      },
      "source": [
        "decoder_target_data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([128,  12,  67,   9,   5,  61,  14,   3,   0,   0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee0DsPNOGlIQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a16cc12a-bccd-49e1-8f1d-52747b1b0d0a"
      },
      "source": [
        "num_encoder_tokens = max([max(i) for i in encoder_input_data])\n",
        "num_decoder_tokens = max([max(i) for i in decoder_input_data])\n",
        "print(num_encoder_tokens + 1)\n",
        "print(num_decoder_tokens + 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5002\n",
            "5002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpPvEjUTFk7r"
      },
      "source": [
        "# Seq2Seq Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O0GIPGEFphk"
      },
      "source": [
        "## Model hipermarameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aLLB0koGn6c"
      },
      "source": [
        "num_encoder_tokens = max([max(i) for i in encoder_input_data]) + 1\n",
        "num_decoder_tokens = max([max(i) for i in decoder_input_data]) + 1\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 1000  # Number of samples to train on."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI0pzU-WFsts"
      },
      "source": [
        "## Model construction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDpH4V6JGp9W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4015
        },
        "outputId": "2788b7a3-f36f-4889-8f95-dd1f515ffd2d"
      },
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "encoder_embedding = tf.keras.layers.Embedding(num_encoder_tokens, latent_dim, mask_zero=True)(encoder_inputs)\n",
        "enconder_outputs, state_h, state_c = tf.keras.layers.LSTM(latent_dim,\n",
        "                           return_state=True)(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "decoder_embedding = tf.keras.layers.Embedding(num_decoder_tokens, latent_dim, mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = tf.keras.layers.Dense(num_decoder_tokens, activation='softmax')\n",
        "output = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
        "\n",
        "# Compile & run training\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "# Note that `decoder_target_data` needs to be one-hot encoded,\n",
        "# rather than sequences of integers like `decoder_input_data`!\n",
        "model.summary()\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 256)    1280512     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 256)    1280512     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 256), (None, 525312      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 5002)   1285514     lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 4,897,162\n",
            "Trainable params: 4,897,162\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 39859 samples, validate on 9965 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Epoch 1/100\n",
            " - 32s - loss: 4.1890 - val_loss: 3.9365\n",
            "Epoch 2/100\n",
            " - 28s - loss: 3.7989 - val_loss: 3.7922\n",
            "Epoch 3/100\n",
            " - 29s - loss: 3.6653 - val_loss: 3.7224\n",
            "Epoch 4/100\n",
            " - 28s - loss: 3.5749 - val_loss: 3.6916\n",
            "Epoch 5/100\n",
            " - 28s - loss: 3.5011 - val_loss: 3.6711\n",
            "Epoch 6/100\n",
            " - 29s - loss: 3.4355 - val_loss: 3.6656\n",
            "Epoch 7/100\n",
            " - 28s - loss: 3.3761 - val_loss: 3.6702\n",
            "Epoch 8/100\n",
            " - 28s - loss: 3.3191 - val_loss: 3.6839\n",
            "Epoch 9/100\n",
            " - 29s - loss: 3.2610 - val_loss: 3.6961\n",
            "Epoch 10/100\n",
            " - 29s - loss: 3.1954 - val_loss: 3.7203\n",
            "Epoch 11/100\n",
            " - 28s - loss: 3.1311 - val_loss: 3.7472\n",
            "Epoch 12/100\n",
            " - 29s - loss: 3.0691 - val_loss: 3.7935\n",
            "Epoch 13/100\n",
            " - 28s - loss: 3.0060 - val_loss: 3.8345\n",
            "Epoch 14/100\n",
            " - 28s - loss: 2.9359 - val_loss: 3.8726\n",
            "Epoch 15/100\n",
            " - 28s - loss: 2.8643 - val_loss: 3.9325\n",
            "Epoch 16/100\n",
            " - 27s - loss: 2.7890 - val_loss: 3.9877\n",
            "Epoch 17/100\n",
            " - 28s - loss: 2.7127 - val_loss: 4.0510\n",
            "Epoch 18/100\n",
            " - 28s - loss: 2.6386 - val_loss: 4.1270\n",
            "Epoch 19/100\n",
            " - 27s - loss: 2.5724 - val_loss: 4.1918\n",
            "Epoch 20/100\n",
            " - 28s - loss: 2.5107 - val_loss: 4.2679\n",
            "Epoch 21/100\n",
            " - 29s - loss: 2.4503 - val_loss: 4.3321\n",
            "Epoch 22/100\n",
            " - 27s - loss: 2.3917 - val_loss: 4.4046\n",
            "Epoch 23/100\n",
            " - 29s - loss: 2.3381 - val_loss: 4.4678\n",
            "Epoch 24/100\n",
            " - 27s - loss: 2.2894 - val_loss: 4.5415\n",
            "Epoch 25/100\n",
            " - 27s - loss: 2.2398 - val_loss: 4.6070\n",
            "Epoch 26/100\n",
            " - 28s - loss: 2.1981 - val_loss: 4.6730\n",
            "Epoch 27/100\n",
            " - 27s - loss: 2.1517 - val_loss: 4.7275\n",
            "Epoch 28/100\n",
            " - 27s - loss: 2.1063 - val_loss: 4.7984\n",
            "Epoch 29/100\n",
            " - 28s - loss: 2.0577 - val_loss: 4.8568\n",
            "Epoch 30/100\n",
            " - 27s - loss: 2.0094 - val_loss: 4.9173\n",
            "Epoch 31/100\n",
            " - 27s - loss: 1.9548 - val_loss: 4.9627\n",
            "Epoch 32/100\n",
            " - 29s - loss: 1.8878 - val_loss: 5.0334\n",
            "Epoch 33/100\n",
            " - 27s - loss: 1.8410 - val_loss: 5.1167\n",
            "Epoch 34/100\n",
            " - 28s - loss: 1.8059 - val_loss: 5.1590\n",
            "Epoch 35/100\n",
            " - 28s - loss: 1.7728 - val_loss: 5.2200\n",
            "Epoch 36/100\n",
            " - 27s - loss: 1.7423 - val_loss: 5.2531\n",
            "Epoch 37/100\n",
            " - 27s - loss: 1.7129 - val_loss: 5.2997\n",
            "Epoch 38/100\n",
            " - 28s - loss: 1.6830 - val_loss: 5.3358\n",
            "Epoch 39/100\n",
            " - 27s - loss: 1.6512 - val_loss: 5.3743\n",
            "Epoch 40/100\n",
            " - 28s - loss: 1.6194 - val_loss: 5.4241\n",
            "Epoch 41/100\n",
            " - 28s - loss: 1.5915 - val_loss: 5.4644\n",
            "Epoch 42/100\n",
            " - 27s - loss: 1.5637 - val_loss: 5.5005\n",
            "Epoch 43/100\n",
            " - 29s - loss: 1.5386 - val_loss: 5.5443\n",
            "Epoch 44/100\n",
            " - 28s - loss: 1.5151 - val_loss: 5.5778\n",
            "Epoch 45/100\n",
            " - 28s - loss: 1.4890 - val_loss: 5.6220\n",
            "Epoch 46/100\n",
            " - 29s - loss: 1.4649 - val_loss: 5.6731\n",
            "Epoch 47/100\n",
            " - 28s - loss: 1.4441 - val_loss: 5.7210\n",
            "Epoch 48/100\n",
            " - 27s - loss: 1.4255 - val_loss: 5.7520\n",
            "Epoch 49/100\n",
            " - 29s - loss: 1.4094 - val_loss: 5.7949\n",
            "Epoch 50/100\n",
            " - 28s - loss: 1.3916 - val_loss: 5.8355\n",
            "Epoch 51/100\n",
            " - 28s - loss: 1.3722 - val_loss: 5.8647\n",
            "Epoch 52/100\n",
            " - 29s - loss: 1.3594 - val_loss: 5.8842\n",
            "Epoch 53/100\n",
            " - 28s - loss: 1.3489 - val_loss: 5.9001\n",
            "Epoch 54/100\n",
            " - 29s - loss: 1.3350 - val_loss: 5.9273\n",
            "Epoch 55/100\n",
            " - 29s - loss: 1.3221 - val_loss: 5.9438\n",
            "Epoch 56/100\n",
            " - 29s - loss: 1.3107 - val_loss: 5.9677\n",
            "Epoch 57/100\n",
            " - 28s - loss: 1.2980 - val_loss: 5.9594\n",
            "Epoch 58/100\n",
            " - 29s - loss: 1.2837 - val_loss: 5.9842\n",
            "Epoch 59/100\n",
            " - 28s - loss: 1.2711 - val_loss: 5.9828\n",
            "Epoch 60/100\n",
            " - 28s - loss: 1.2583 - val_loss: 5.9896\n",
            "Epoch 61/100\n",
            " - 28s - loss: 1.2467 - val_loss: 5.9936\n",
            "Epoch 62/100\n",
            " - 28s - loss: 1.2351 - val_loss: 6.0139\n",
            "Epoch 63/100\n",
            " - 29s - loss: 1.2234 - val_loss: 6.0112\n",
            "Epoch 64/100\n",
            " - 28s - loss: 1.2124 - val_loss: 6.0165\n",
            "Epoch 65/100\n",
            " - 29s - loss: 1.2011 - val_loss: 6.0027\n",
            "Epoch 66/100\n",
            " - 29s - loss: 1.1905 - val_loss: 6.0162\n",
            "Epoch 67/100\n",
            " - 28s - loss: 1.1791 - val_loss: 6.0217\n",
            "Epoch 68/100\n",
            " - 28s - loss: 1.1711 - val_loss: 6.0303\n",
            "Epoch 69/100\n",
            " - 29s - loss: 1.1592 - val_loss: 6.0383\n",
            "Epoch 70/100\n",
            " - 28s - loss: 1.1494 - val_loss: 6.0239\n",
            "Epoch 71/100\n",
            " - 28s - loss: 1.1401 - val_loss: 6.0323\n",
            "Epoch 72/100\n",
            " - 29s - loss: 1.1322 - val_loss: 6.0324\n",
            "Epoch 73/100\n",
            " - 28s - loss: 1.1231 - val_loss: 6.0257\n",
            "Epoch 74/100\n",
            " - 28s - loss: 1.1164 - val_loss: 6.0438\n",
            "Epoch 75/100\n",
            " - 29s - loss: 1.1071 - val_loss: 6.0339\n",
            "Epoch 76/100\n",
            " - 29s - loss: 1.0998 - val_loss: 6.0432\n",
            "Epoch 77/100\n",
            " - 29s - loss: 1.0915 - val_loss: 6.0438\n",
            "Epoch 78/100\n",
            " - 28s - loss: 1.0865 - val_loss: 6.0530\n",
            "Epoch 79/100\n",
            " - 28s - loss: 1.0791 - val_loss: 6.0499\n",
            "Epoch 80/100\n",
            " - 29s - loss: 1.0713 - val_loss: 6.0555\n",
            "Epoch 81/100\n",
            " - 28s - loss: 1.0654 - val_loss: 6.0465\n",
            "Epoch 82/100\n",
            " - 28s - loss: 1.0601 - val_loss: 6.0562\n",
            "Epoch 83/100\n",
            " - 29s - loss: 1.0538 - val_loss: 6.0512\n",
            "Epoch 84/100\n",
            " - 28s - loss: 1.0473 - val_loss: 6.0597\n",
            "Epoch 85/100\n",
            " - 28s - loss: 1.0406 - val_loss: 6.0623\n",
            "Epoch 86/100\n",
            " - 29s - loss: 1.0354 - val_loss: 6.0770\n",
            "Epoch 87/100\n",
            " - 29s - loss: 1.0293 - val_loss: 6.0801\n",
            "Epoch 88/100\n",
            " - 29s - loss: 1.0237 - val_loss: 6.0809\n",
            "Epoch 89/100\n",
            " - 29s - loss: 1.0191 - val_loss: 6.0818\n",
            "Epoch 90/100\n",
            " - 28s - loss: 1.0142 - val_loss: 6.0908\n",
            "Epoch 91/100\n",
            " - 28s - loss: 1.0078 - val_loss: 6.1033\n",
            "Epoch 92/100\n",
            " - 29s - loss: 1.0028 - val_loss: 6.1034\n",
            "Epoch 93/100\n",
            " - 28s - loss: 0.9998 - val_loss: 6.1138\n",
            "Epoch 94/100\n",
            " - 28s - loss: 0.9951 - val_loss: 6.1248\n",
            "Epoch 95/100\n",
            " - 29s - loss: 0.9886 - val_loss: 6.1186\n",
            "Epoch 96/100\n",
            " - 28s - loss: 0.9883 - val_loss: 6.1317\n",
            "Epoch 97/100\n",
            " - 29s - loss: 0.9820 - val_loss: 6.1172\n",
            "Epoch 98/100\n",
            " - 29s - loss: 0.9789 - val_loss: 6.1311\n",
            "Epoch 99/100\n",
            " - 29s - loss: 0.9744 - val_loss: 6.1328\n",
            "Epoch 100/100\n",
            " - 29s - loss: 0.9711 - val_loss: 6.1407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3fffdf0630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njwRqESBFyvk"
      },
      "source": [
        "## Inference model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukr7XyIEQC6f"
      },
      "source": [
        "max_question_len = encoder_input_data.shape[1]\n",
        "max_answer_len = decoder_input_data.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU0CqJINJ5uk"
      },
      "source": [
        "def make_inference_models():    \n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=(latent_dim ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=(latent_dim ,))\n",
        "    \n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYu8UhUOJ9vs"
      },
      "source": [
        "def str_to_tokens(sentence):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append(word_dict.get(word, word_dict['unk']) )\n",
        "    return tokens_list + [0] * (max_question_len - len(tokens_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjtaOEXaF2th"
      },
      "source": [
        "## Talking with the bot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61bvePv8J_6A"
      },
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10):\n",
        "    question = input( 'You: ' )\n",
        "    states_values = enc_model.predict(str_to_tokens(question))\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = word_dict['sos']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition:\n",
        "        if question == 'Goodbye':\n",
        "            stop_condition = True\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values )\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in word_dict.items():\n",
        "            #print(sampled_word)\n",
        "            if sampled_word_index == index:\n",
        "                if word != 'eos':\n",
        "                    decoded_translation += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'eos' or len(decoded_translation.split()) > max_answer_len:\n",
        "            stop_condition = True\n",
        "        \n",
        "            \n",
        "        empty_target_seq = np.zeros((1 , 1))  \n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c] \n",
        "    print('KerasBot: ' + decoded_translation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNZjhDALGNkg"
      },
      "source": [
        "## Model save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgQdESdasNdc"
      },
      "source": [
        "model.save('best_bot_version2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WirNNzqbIHX"
      },
      "source": [
        "model.save_weights(\"model_bot_version2.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}